{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pdpipe as pdp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    issues = pd.read_csv('../data/original/issues.csv')\n",
    "    transitions = pd.read_csv('../data/original/transitions.csv')\n",
    "    daycounts = pd.read_csv('../data/original/daycounts.csv')\n",
    "except FileNotFoundError as e:\n",
    "    print(e.strerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all columns are present \n",
    "if (len(list(issues)) != 17) & (len(list(transitions)) != 24) & (len(list(daycounts)) != 3):\n",
    "    print('Some columns might be missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting values per dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_datepart(df, column_name, drop=True):\n",
    "    \"\"\"\n",
    "    Helper function that adds columns relevant to a date\n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): Dataframe in which the column is present.\n",
    "        column_name (string): Name of the column you want to change.\n",
    "        drop (boolean): To keep or to drop \"old\" column\n",
    "        time (boolean): Include hours and minutes as well\n",
    "\n",
    "    Returns:\n",
    "        columns: The newly created columns are added to the dataframe.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    fld = df[column_name]\n",
    "    fld_dtype = fld.dtype\n",
    "    \n",
    "    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype): # change dtype if necessary\n",
    "        fld_dtype = np.datetime64\n",
    "\n",
    "    if not np.issubdtype(fld_dtype, np.datetime64):\n",
    "        df[column_name] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n",
    "    targ_pre = re.sub('[Dd]ate$', '', column_name)\n",
    "    \n",
    "    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n",
    "            'Is_month_end', 'Is_month_start', 'Is_quarter_end', \n",
    "            'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "    for column in attr: #\n",
    "        df[targ_pre + column] = getattr(fld.dt, column.lower())\n",
    "    df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n",
    "    if drop: \n",
    "        df.drop(column_name, axis=1, inplace=True)\n",
    "        \n",
    "        \n",
    "def from_days_to_hours(dataframe, column):\n",
    "    \"\"\"\n",
    "    Change the number of days to hours\n",
    "    \n",
    "    Parameters:\n",
    "    dataframe (dataframe): name of dataframe that contains the column to change\n",
    "    column (string): name of the column to change\n",
    "    \n",
    "    Return:\n",
    "    difference_in_hours (list): list containing the number of hours it took\n",
    "                                for a key to be resolved\n",
    "    \"\"\"\n",
    "    \n",
    "    difference_in_hours = []\n",
    "    \n",
    "    for i in dataframe[column]:\n",
    "        days, seconds = i.days, i.seconds\n",
    "        difference_in_hours.append(days * 24 + seconds // 3600)\n",
    "    return difference_in_hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data transformation\n",
    "issues['created'] = pd.to_datetime(issues['created']) # from object to datetime\n",
    "issues['resolutiondate'] = pd.to_datetime(issues['resolutiondate']) # from object to datetime\n",
    "\n",
    "# data creation\n",
    "issues['time_needed'] = issues['resolutiondate'] - issues['created'] # time needed to resolve\n",
    "issues['days_needed'] = issues['time_needed'].dt.days # days needed to resolve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues['hours_needed'] = issues['resolutiondate'] - issues['created']\n",
    "issues['hours_needed'] = from_days_to_hours(issues, 'hours_needed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues['created_date'] = issues['created'] # create will be removed after calling add_datepart\n",
    "add_datepart(issues, 'created') # extract external information from date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transitions dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating functions for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(column):\n",
    "    \"\"\"\n",
    "    Create the dataset that will later be used to train a decision tree\n",
    "    \n",
    "    Parameter:\n",
    "    column (string): will function as a filter\n",
    "    \n",
    "    Return:\n",
    "    dataset (dataframe): dataframe that will later be used to train a model\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    dataset = transitions_finished.loc[transitions_finished['from_status'] == column]\n",
    "    \n",
    "    dataset_name = dataset[['key', 'days_since_open', 'from_status', 'to_status', \n",
    "                            'days_in_from_status', 'who', 'description_length', \n",
    "                            'comment_count', 'issue_type', 'vote_count',\n",
    "                            'watch_count', 'priority']]\n",
    "    \n",
    "    return dataset_name\n",
    "\n",
    "\n",
    "def create_dt(dataframe):\n",
    "    \"\"\"\n",
    "    Create a decision tree algorithm for a specific problem\n",
    "    \n",
    "    Parameter:\n",
    "    dataframe (dataframe): dataframe used to train the model\n",
    "    \n",
    "    Return:\n",
    "    name_decisiontree (algorithm): trained algorithm for a specific problem\n",
    "    \"\"\"\n",
    "    \n",
    "    # Feautres for prediction\n",
    "    X = dataframe[['who', 'description_length', 'comment_count', \n",
    "                   'issue_type', 'vote_count', 'watch_count',\n",
    "                   'priority','watch_count']].to_numpy() \n",
    "    # Target variable\n",
    "    y = dataframe['to_status'].to_numpy() \n",
    "    \n",
    "    # 80% training and 20% test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    \n",
    "    name_decisiontree = DecisionTreeClassifier()\n",
    "    name_decisiontree = name_decisiontree.fit(X_train,y_train)\n",
    "    return name_decisiontree\n",
    "\n",
    "\n",
    "def make_prediction(dataframe, last_status):\n",
    "    \"\"\"\n",
    "    Make a prediction given a dataset and the latest prediction\n",
    "    \n",
    "    Parameter:\n",
    "    datafame (dataframe): daframe that will be used to make a predcition\n",
    "    last_status (string): the last status given in transition\n",
    "    \n",
    "    Return:\n",
    "    The prediciton made on a specific problem\n",
    "    \n",
    "    \"\"\"\n",
    "    if last_status == 'Open':\n",
    "        return open_dt.predict(dataframe)\n",
    "    if last_status == 'Patch Available':\n",
    "        #return patch_available_dt.predict(dataframe)\n",
    "        return ['Resolved']\n",
    "    if last_status == 'In Progress':\n",
    "        return in_progress_dt.predict(dataframe)\n",
    "    else:\n",
    "        return ['Resolved']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning data for machine learning algorithm\n",
    "transitions['who_old'] = transitions['who']\n",
    "lb_make = LabelEncoder()\n",
    "transitions[\"who\"] = lb_make.fit_transform(transitions[\"who\"])\n",
    "transitions[\"issue_type\"] = lb_make.fit_transform(transitions[\"issue_type\"])\n",
    "transitions[\"priority\"] = lb_make.fit_transform(transitions[\"priority\"])\n",
    "\n",
    "# scale data \n",
    "pipeline = pdp.Scale('RobustScaler', ['description_length', 'comment_count', 'vote_count', 'watch_count'])\n",
    "transitions = pipeline(transitions)\n",
    "\n",
    "# only select data on which algorithm can train\n",
    "keys = list(issues.dropna(subset=['resolutiondate'])['key']) # only select finished keys\n",
    "transitions_finished = transitions.loc[transitions['key'].isin(keys)] # select dataframe\n",
    "transitions_finished = transitions_finished.reset_index() # reset index\n",
    "status = list(transitions_finished['from_status'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create status open dataset\n",
    "open_dataset = create_dataset('Open')\n",
    "open_dataset = open_dataset.fillna(0) # description length only na value\n",
    "\n",
    "# create status patch available dataset\n",
    "patch_available_dataset = create_dataset('Patch Available')\n",
    "patch_available_dataset = patch_available_dataset.fillna(0)# description length only na value\n",
    "\n",
    "# create status open dataset\n",
    "in_progress_dataset = create_dataset('In Progress')\n",
    "\n",
    "# create decision tree per dataset  \n",
    "open_dt = create_dt(open_dataset)\n",
    "patch_available_dt = create_dt(patch_available_dataset)\n",
    "in_progress_dt = create_dt(in_progress_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select those keys that still need to be resolved\n",
    "transitions_progress = transitions_progress.loc[~transitions_progress['key'].isin(keys)]\n",
    "# list of keys that still need to be resolved\n",
    "keys_to_process = list(transitions_progress['key'].unique())\n",
    "# select relevant columns\n",
    "transitions_progress = transitions_progress[['key', 'days_since_open', 'from_status', \n",
    "                                                       'to_status', 'days_in_from_status', 'who', 'description_length', 'comment_count',\n",
    "                                                       'issue_type', 'vote_count', 'watch_count', 'priority']]\n",
    "transitions_progress['description_length'] = transitions_progress['description_length'].fillna(0)\n",
    "\n",
    "# store all predictions\n",
    "predicted_actions = []\n",
    "\n",
    "\n",
    "for key in keys_to_process:\n",
    "    actions = []\n",
    "    df = transitions_progress.loc[transitions_progress['key'] == key] \n",
    "\n",
    "    last_status = list(df['to_status'])[-1] # get latest status and go from there\n",
    "    last_row = df.iloc[[-1]] # use latest info \n",
    "    last_row = last_row[['who', 'description_length', 'comment_count', \n",
    "                         'issue_type', 'vote_count', 'watch_count', \n",
    "                         'priority', 'watch_count']] # Features\n",
    "\n",
    "    first_last_status = list(df['to_status'])[-1] # get latest status and go from there\n",
    "    i = 0\n",
    "    while i == 0:\n",
    "        last_status = make_prediction(last_row, last_status)\n",
    "        if first_last_status != last_status[0]:\n",
    "            if last_status[0] == 'Resolved':\n",
    "                actions.append(last_status[0])\n",
    "                i = 1\n",
    "            else:\n",
    "                actions.append(last_status[0])\n",
    "                i = 0\n",
    "        else:\n",
    "            actions.append('Resolved')\n",
    "            i = 1\n",
    "    predicted_actions.append(actions)\n",
    "    \n",
    "# create new dataframe for predicted values\n",
    "df_to_add = {'key' : keys_to_process, 'Predicted_actions' : predicted_actions} \n",
    "df_to_add = pd.DataFrame(df_to_add)\n",
    "\n",
    "# merge prediction dataframe with original issues dataframe\n",
    "issues = issues.merge(df_to_add, how = 'left', on = 'key')\n",
    "issues['Predicted_actions'] = issues['Predicted_actions'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create funtions for feature creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_datetype(dataset, column_name, hours = True):\n",
    "    \"\"\"\n",
    "    Changes the type of a column from object to datetime\n",
    "    \n",
    "    Parameters:\n",
    "        dataset (dataframe): Datasets that contain the column\n",
    "        column_name (str): The column which you want to change.\n",
    "        hours (boolean): Choosing specifc column\n",
    "        \n",
    "    Returns:\n",
    "        column (list): The column which is of type datetime. \n",
    "    \"\"\"\n",
    "    column = pd.to_datetime(dataset[column_name])\n",
    "    if hours:\n",
    "        column = [time.strftime('%H:%m:%S %d-%m-%Y') for time in column]\n",
    "    else:\n",
    "        column = [time.strftime('%d-%m-%Y') for time in column]\n",
    "    column = pd.to_datetime(column)\n",
    "    \n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_info():\n",
    "    \"\"\"\n",
    "    Event info contains information about the number of steps taken and the specifc steps\n",
    " \n",
    "    Returns:\n",
    "    list_steps (list): contains the number of steps taken.\n",
    "    list_steps_taken (list): contains the number of steps taken.\n",
    "    list_status (list): contains information about the steps taken used to be transformed later\n",
    "    complete_time (list): contains the time that was needed per status\n",
    "    \"\"\"\n",
    "        \n",
    "    list_steps = []\n",
    "    list_steps_taken = [] \n",
    "    list_status = [] \n",
    "    complete_time = []\n",
    "\n",
    "    \n",
    "    for key in issues['key']:\n",
    "        df = transitions.loc[transitions['key'] == key] # select relevant data\n",
    "        df = df.loc[df['to_status'] != 'Closed'] # remove where to_status equals 'Closed'\n",
    "        \n",
    "        time = list(df['days_in_from_status'])\n",
    "        status_key = df['from_status'].append(df['to_status']) # get all status in data\n",
    "        status_key = pd.Series(status_key).drop_duplicates().tolist() # remove duplicates, keep order\n",
    "        \n",
    "        list_steps.append(len(status_key))\n",
    "        list_steps_taken.append('-'.join(status_key))\n",
    "        list_status.append(status_key)\n",
    "        complete_time.append(time)\n",
    "        \n",
    "    return list_steps, list_steps_taken, list_status, complete_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary():\n",
    "    \"\"\"\n",
    "    Creates a dictionary of the status keys\n",
    "        \n",
    "    Returns:\n",
    "    status_keys_dict (dict): dictionary containing status plus num and alpha translation.\n",
    "    \"\"\"\n",
    "    alphabet = string.ascii_lowercase # import the alphabet\n",
    "    unique_status_key = list(set(transitions['from_status'].append(transitions['to_status'])))\n",
    "    \n",
    "    status_keys_dict = tuple(enumerate(unique_status_key)) # create a dictionary from unique status values\n",
    "    status_keys_dict = dict((status, num) for num, status in status_keys_dict) # numer dict\n",
    "    status_keys_dict = dict((status, [alphabet[status_keys_dict[status]], \n",
    "                                      status_keys_dict[status]]) for status in status_keys_dict) #alpha dict\n",
    "    \n",
    "    return status_keys_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_taken(dictionary, status_event_value):\n",
    "    \"\"\"\n",
    "    Given a dictionary it will create a string of the path taken\n",
    "        \n",
    "    Returns:\n",
    "    path_taken_total (list): containing information about the path taken.\n",
    "    \"\"\"\n",
    "    path_taken_total = []\n",
    "    \n",
    "    for events in status_event:\n",
    "        path_taken = [str(dictionary[event][0]) for event in events]\n",
    "        path_taken_total.append(path_taken)\n",
    "    \n",
    "    return path_taken_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_per_status(dictionary):\n",
    "    \"\"\"\n",
    "    Extract the time it took to go from one status to another\n",
    "    \n",
    "    Parameters:\n",
    "    dictionary (dict): dictionary containing the translation of status\n",
    "    \n",
    "    Returns:\n",
    "    time_steps_list (list): List containing the time it took per step\n",
    "    \"\"\"\n",
    "\n",
    "    time_steps_list = []\n",
    "\n",
    "    for key in issues['key']:\n",
    "        df = transitions.loc[transitions['key'] == key] # select relevant data\n",
    "        df = df.loc[df['to_status'] != 'Closed'] # remove where to_status equals 'Closed'\n",
    "\n",
    "        df['days_in_from_status'] = df['days_in_from_status'].fillna(0)\n",
    "        steps_transition = [value.split(\" to \") for value in df['transition']]\n",
    "        list_of_steps = [[step for step in steps] for steps in steps_transition]\n",
    "        list_of_steps = ['_'.join(steps) for steps in list_of_steps]\n",
    "        status_list = list(df['days_in_from_status'])\n",
    "        time_steps_list.append({list_of_steps[value] : status_list[value] for value in range(len(df))})\n",
    "    \n",
    "    return time_steps_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_avg_time():\n",
    "    \"\"\"\n",
    "    Returns the avg time it took per step\n",
    "    \n",
    "    avg_transition (dataframe): containing the time it took to finish a task\n",
    "                                time will be represented in avg and std\n",
    "    \"\"\"\n",
    "    avg = []\n",
    "    for prio in priority:\n",
    "        for issue in issue_type:\n",
    "            for value in transition:\n",
    "                test = df_plus_steps.loc[df[value] != 0]\n",
    "                test = test.loc[df_plus_steps['priority'] == prio]\n",
    "                avg.append([prio,issue, value, test[value].mean(), test[value].std()])\n",
    "    avg_transition = pd.DataFrame(avg, columns = ['priority', 'issue_type', 'transition', 'avg', 'std'])\n",
    "    \n",
    "    return avg_transition.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## who worked on the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "issue_key = list(issues['key'].unique())\n",
    "\n",
    "most_assigned = {'cutting' : 1,\n",
    "                'thiru_mg': 2,\n",
    "                'tomwhite': 3,\n",
    "                'hammer': 4,\n",
    "                'massie': 5,\n",
    "                'rdblue': 6,\n",
    "                'dcreager': 7,\n",
    "                'scott_carey': 8,\n",
    "                'philip': 9,\n",
    "                'busbey': 10, \n",
    "                'sbanacho': 11,\n",
    "                'nielsbasjes': 12,\n",
    "                'sacharya': 13}\n",
    "\n",
    "working_people = list(most_assigned.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_workes = []\n",
    "for key in issue_key:\n",
    "    df = transitions.loc[transitions['key'] == key]\n",
    "    who_list = list(set(df['who_old']))\n",
    "    \n",
    "    people_value = []\n",
    "    for i in who_list:\n",
    "        if i in working_people:\n",
    "            value = most_assigned[i]\n",
    "            people_value.append(str(value)) # if in top 13 \n",
    "        else:\n",
    "            people_value.append(str(14)) # if not in top 13\n",
    "    issue_workes.append(people_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "worked_on_project = pd.DataFrame(issue_workes, columns = ['worker_1', 'worker_2', 'worker_3', 'worker_4'] ).fillna(0)\n",
    "issues = pd.concat([issues, worked_on_project], axis=1, sort=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing datetypes\n",
    "transitions['when'] = change_datetype(transitions, 'when')\n",
    "transitions['updated'] = change_datetype(transitions, 'updated')\n",
    "transitions['created'] = change_datetype(transitions, 'created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill na values in from_status with Non-existent\n",
    "transitions['from_status'] = transitions['from_status'].fillna('Non-existent')\n",
    "steps, steps_taken, status_event, time = get_event_info() # get information about each event\n",
    "\n",
    "issues['number_of_steps'] = steps # adding steps to issues dataframe\n",
    "issues['steps_taken'] = steps_taken  # adding steps taken to issues dataframe\n",
    "\n",
    "time_needed_df = pd.DataFrame.from_records(time).fillna(0) # put each time slot in a individual column\n",
    "issues = pd.concat([issues, time_needed_df], axis=1, sort=False) # add time slot data to issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining existing status events with the predicted status event, making it a complete event\n",
    "issues['status_event'] = status_event\n",
    "\n",
    "combination_list = []\n",
    "\n",
    "for key in range(len(issues)):\n",
    "    if issues['Predicted_actions'][key] != 0:\n",
    "        # if status is not yet complete add prediction\n",
    "        steps_already_taken = issues['status_event'][key]\n",
    "        steps_predicted = issues['Predicted_actions'][key]\n",
    "        complete_status_event = steps_already_taken + steps_predicted\n",
    "        combination_list.append(complete_status_event)\n",
    "    else:\n",
    "        # if already complete use that status\n",
    "        combination_list.append(issues['status_event'][key])\n",
    "        \n",
    "        \n",
    "# add combination to dataframe\n",
    "issues['steps_taken_combined'] = combination_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of the steps that can be taken\n",
    "keys_dictionary = create_dictionary()\n",
    "\n",
    "# make a list of the steps that have been taken\n",
    "path_taken_list = path_taken(keys_dictionary, list(issues['steps_taken_combined']))\n",
    "df_01 = pd.DataFrame(list(issues['steps_taken_combined']),\n",
    "                     columns = ['step0', 'step1','step2','step3','step4','step5', 'step6'])\n",
    "\n",
    "# combine the two dataset (issues and path_taken) together \n",
    "result = pd.concat([issues, df_01], axis=1, sort=False) # adding df to issues dataset\n",
    "result['number_of_steps'] = [len(i) for i in result['steps_taken_combined']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = MultiLabelBinarizer() # initialize MultiLabelBinarizer object\n",
    "test = one_hot.fit_transform(list(issues['steps_taken_combined'])) # one-hot encode data\n",
    "df = pd.DataFrame(test, columns=one_hot.classes_) # change column names to relevant columns\n",
    "\n",
    "df_plus_status = pd.concat([result, df], axis=1, sort=False) # adding df to issues dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_status = create_dictionary()\n",
    "time_taken_per_step = get_time_per_status(dictionary_status)\n",
    "df = pd.DataFrame(time_taken_per_step) \n",
    "df = df.fillna(0)\n",
    "df_plus_steps = pd.concat([df_plus_status, df], axis=1, sort=False) # adding df to issues dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "priority = list(result['priority'].unique()) # unique values for priority\n",
    "issue_type = list(result['issue_type'].unique()) # unique values for issue_type\n",
    "\n",
    "# all possible combinations in transition\n",
    "transition = ['Open_Patch Available','Patch Available_In Progress', 'Open_Resolved',\n",
    "              'Patch Available_Resolved', 'Resolved_Reopened', 'Reopened_Resolved', \n",
    "              'In Progress_Patch Available', 'Patch Available_Open', 'Reopened_Patch Available', \n",
    "              'Open_In Progress', 'In Progress_Resolved', 'In Progress_Open']\n",
    "\n",
    "\n",
    "# get avg time needed per transition\n",
    "avg_transition = create_avg_time()\n",
    "\n",
    "# create empty dataframes\n",
    "dataset_avg = pd.DataFrame()\n",
    "dataset_std = pd.DataFrame()\n",
    "\n",
    "# create dataframe corresponding with the issue_type and priority of an issue\n",
    "for i in range(len(df)):\n",
    "    to_append_avg = avg_transition.loc[(avg_transition['priority'] == df_plus_steps['priority'][i]) & \n",
    "                               (avg_transition['issue_type'] == df_plus_steps['issue_type'][i])][['avg', 'transition']]\n",
    "    to_append_avg = to_append_avg.set_index('transition').T\n",
    "    \n",
    "    to_append_std = avg_transition.loc[(avg_transition['priority'] == df_plus_steps['priority'][i]) & \n",
    "                               (avg_transition['issue_type'] == df_plus_steps['issue_type'][i])][['avg', 'transition']]\n",
    "    to_append_std = to_append_std.set_index('transition').T\n",
    "    \n",
    "    dataset_avg = dataset_avg.append(to_append_avg)\n",
    "    dataset_std = dataset_std.append(to_append_std)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_std = dataset_std.reset_index() # reset index\n",
    "dataset_avg = dataset_avg.reset_index() # reset index\n",
    "\n",
    "df_plus_avg = pd.concat([df_plus_steps, dataset_avg], axis=1, sort=False) # adding df to issues dataset\n",
    "df_plus_std = pd.concat([df_plus_avg, dataset_std], axis=1, sort=False) # adding df to issues dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daycounts dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chagne date type\n",
    "daycounts['day'] = change_datetype(daycounts, 'day')\n",
    "issues['created_date_T'] = change_datetype(issues, 'created_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of projects that are in progress at time of creation new issue\n",
    "# only look at the project inprogress or patch_available as well?\n",
    "current_projects = []\n",
    "\n",
    "for issue in range(len(issues)):\n",
    "    try:\n",
    "        start = issues['created_date_T'][issue]        \n",
    "        start_df = daycounts.loc[(daycounts['day'] == start)]\n",
    "\n",
    "        # get number of projects currently \"in progress\"\n",
    "        projects_in_progress = list(start_df.loc[start_df['status'] == 'In Progress']['count'])[0]\n",
    "        # add value to list\n",
    "        current_projects.append(projects_in_progress)\n",
    "        \n",
    "    except:\n",
    "        # if date isn't available add 0 project in progress\n",
    "        current_projects.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plus_std['projects_in_progress'] = current_projects # projects in progress at start of issue\n",
    "df_plus_std['steps_taken_combined'] = ['_'.join(i) for i in df_plus_std['steps_taken_combined']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill NaN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is likely that the descirptions with the NA value has no comments, therefore 0\n",
    "df_plus_std['description_length'] = df_plus_std['description_length'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store final datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plus_std.to_csv('../data/transformed/pipeline_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
